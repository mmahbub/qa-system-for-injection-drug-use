{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import string\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "import seaborn as sns\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_cat = {\n",
    "           'active/historical':'active/historical use',\n",
    "           'drug names':'drug names',\n",
    "           'existence of idu':'existence of IDU',\n",
    "           'last use':'last use',\n",
    "           'occasional/daily':'frequency of use',\n",
    "           'needle sharing':'risky needle-using behavior',\n",
    "           'skin popping':'skin popping',\n",
    "           'evidence track marks': 'visible signs of IDU', \n",
    "           'ssp':'harm reduction interventions',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = 'clinicalbert'\n",
    "m = AutoTokenizer.from_pretrained(\n",
    "    f\"/mnt/pfb_rvdata/mahbub/pre_trained_models/{model}/\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = m.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_jan = pd.read_csv('/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/train_test_jan_postvalidation.csv')\n",
    "df_feb = pd.read_csv('/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_feb_postvalidation.csv')\n",
    "df_nov = pd.read_csv('/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_nov_postvalidation.csv')\n",
    "\n",
    "print(df_jan.shape)\n",
    "print(df_feb.shape)\n",
    "print(df_nov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jan['CONTEXT_LEN_WORDS'] = df_jan['CONTEXT'].progress_apply(count_words)\n",
    "df_feb['CONTEXT_LEN_WORDS'] = df_feb['CONTEXT'].progress_apply(count_words) \n",
    "df_nov['CONTEXT_LEN_WORDS'] = df_nov['CONTEXT'].progress_apply(count_words) \n",
    "\n",
    "df_jan['QUESTION_LEN_WORDS'] = df_jan['QUESTION'].progress_apply(count_words)\n",
    "df_feb['QUESTION_LEN_WORDS'] = df_feb['QUESTION'].progress_apply(count_words) \n",
    "df_nov['QUESTION_LEN_WORDS'] = df_nov['QUESTION'].progress_apply(count_words) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_jan[['VALIDATED_ANSWER']].sample(10) #.iloc[9614].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_jan = json.load(open(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/train_jan.json\"))\n",
    "val_jan   = json.load(open(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/val_jan.json\"))\n",
    "test_jan  = json.load(open(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_jan.json\"))\n",
    "test_feb  = json.load(open(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_feb.json\"))\n",
    "test_nov  = json.load(open(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_nov.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_jan['data'][0]['paragraphs'])\n",
    "\n",
    "assert len(df_jan[df_jan['SPLIT']=='TRAIN']) == len(train_jan['data'])\n",
    "assert len(df_jan[df_jan['SPLIT']=='VAL']) == len(val_jan['data'])\n",
    "assert len(df_jan[df_jan['SPLIT']=='TEST']) == len(test_jan['data'])\n",
    "assert len(df_feb) == len(test_feb['data'])\n",
    "assert len(df_nov) == len(test_nov['data'])\n",
    "\n",
    "def get_answerls_from_json(json_file):\n",
    "    answer_list = []\n",
    "    for x in json_file['data']:\n",
    "        assert len(x) == 2\n",
    "        assert len(x['paragraphs']) == 1\n",
    "        para = x['paragraphs'][0]\n",
    "        assert len(para['qas']) == 1\n",
    "        answer = para['qas'][0]['answers']\n",
    "        assert len(answer) == 1\n",
    "        answer = answer[0]['text']\n",
    "        answer_list.append(answer)\n",
    "    print(len(answer_list))\n",
    "    return answer_list\n",
    "\n",
    "train_jan_answerls = get_answerls_from_json(train_jan)\n",
    "val_jan_answerls   = get_answerls_from_json(val_jan)\n",
    "test_jan_answerls  = get_answerls_from_json(test_jan)\n",
    "test_feb_answerls  = get_answerls_from_json(test_feb)\n",
    "test_nov_answerls  = get_answerls_from_json(test_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_jan_answerls+val_jan_answerls+test_jan_answerls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jan_ansLen = [count_words(ans) for ans in train_jan_answerls+val_jan_answerls+test_jan_answerls]\n",
    "feb_ansLen = [count_words(ans) for ans in test_feb_answerls]\n",
    "nov_ansLen = [count_words(ans) for ans in test_nov_answerls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max([count_words(x) for x in test_jan_answerls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Mean: \", np.mean(df_jan['QUESTION_LEN_WORDS']))\n",
    "# print(\"Median: \", np.median(df_jan['QUESTION_LEN_WORDS']))\n",
    "# print(\"Max: \", np.max(df_jan['QUESTION_LEN_WORDS']))\n",
    "\n",
    "# print(\"Mean: \", np.mean(jan_ansLen))\n",
    "# print(\"Median: \", np.median(jan_ansLen))\n",
    "# print(\"Max: \", np.max(jan_ansLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# statistics.mean(jan_ansLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Mean: \", np.mean(df_feb['QUESTION_LEN_WORDS']))\n",
    "# print(\"Median: \", np.median(df_feb['QUESTION_LEN_WORDS']))\n",
    "# print(\"Max: \", np.max(df_feb['QUESTION_LEN_WORDS']))\n",
    "\n",
    "# print(\"Mean: \", np.mean(feb_ansLen))\n",
    "# print(\"Median: \", np.median(feb_ansLen))\n",
    "# print(\"Max: \", np.max(feb_ansLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Mean: \", np.mean(df_nov['QUESTION_LEN_WORDS']))\n",
    "# print(\"Median: \", np.median(df_nov['QUESTION_LEN_WORDS']))\n",
    "# print(\"Max: \", np.max(df_nov['QUESTION_LEN_WORDS']))\n",
    "\n",
    "# print(\"Mean: \", np.mean(nov_ansLen))\n",
    "# print(\"Median: \", np.median(nov_ansLen))\n",
    "# print(\"Max: \", np.max(nov_ansLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(set(df_nov['TIUDocumentSID'])), len(df_nov), len(set(df_nov['PatientICN']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = ['#e69d00', '#56b3e9', '#009e74', \n",
    "          '#f0e442', '#0071b2', '#808080',\n",
    "          '#d55c00', '#cc79a7', '#000000']\n",
    "map_cat = {\n",
    "           'active/historical':'active/historical use',\n",
    "           'drug names':'drug names',\n",
    "           'existence of idu':'existence of IDU',\n",
    "           'last use':'last use',\n",
    "           'occasional/daily':'frequency of use',\n",
    "           'needle sharing':'risky needle-using behavior',\n",
    "           'skin popping':'skin popping',\n",
    "           'evidence track marks': 'visible signs of IDU', \n",
    "           'ssp':'harm reduction interventions',\n",
    "}\n",
    "\n",
    "cat_to_color = dict(zip(map_cat.values(), colors))\n",
    "\n",
    "def plot_categories_dist(df, month):\n",
    "    \n",
    "    df['CAT'] = df['CATEGORIES'].map(map_cat)\n",
    "\n",
    "#     random.seed(1)\n",
    "#     colors = random.sample(sns.color_palette('pastel'),9)\n",
    "    \n",
    "    labels = [x for x in df['CAT'].value_counts().keys()]\n",
    "    \n",
    "    colors = [cat_to_color[k] for k in df['CAT'].value_counts().keys()]\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(8,22))\n",
    "    explode = (0.01,0.01,0.02,0.03,0.04,0.05,0.2,0.3,\n",
    "               .5\n",
    "              )\n",
    "    plt.pie(df['CAT'].value_counts().values,\n",
    "            labels = labels,\n",
    "            explode=explode,\n",
    "            colors=colors,\n",
    "            autopct='%1.2f%%',\n",
    "            shadow=False,\n",
    "            startangle=45,\n",
    "            textprops={'fontsize':12})\n",
    "    # plt.xticks(size=25)\n",
    "    \n",
    "#     fig.savefig(f\"/mnt/pfb_rvdata/mahbub/EGRESS/FIGURES/pie_{month}.svg\",\n",
    "#             format='svg', dpi=600,\n",
    "#             bbox_inches='tight'\n",
    "#            )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_binned_score_len(df, column_name, bins, xlab):\n",
    "    df['COUNT'] = [1]*len(df)\n",
    "    col = ['EM','F1','Precision','Recall',\n",
    "           'COUNT', column_name]\n",
    "    colors= ['#e69f00', '#000000']\n",
    "    \n",
    "    df_binned     = df.groupby(pd.cut(df[column_name],bins=bins))[col].mean()\n",
    "    df_binned_sum = df.groupby(pd.cut(df[column_name],bins=bins))[col].sum()\n",
    "\n",
    "    assert (df_binned.index == df_binned_sum.index).all()\n",
    "    assert df_binned.shape == df_binned_sum.shape\n",
    "    \n",
    "    df_binned.dropna(inplace=True)\n",
    "    df_binned_sum.dropna(inplace=True)\n",
    "    df_binned_sum = df_binned_sum[df_binned_sum['COUNT']!=0]\n",
    "    \n",
    "    assert df_binned.shape == df_binned_sum.shape\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(9,8))\n",
    "    \n",
    "    plt.plot(range(df_binned.shape[0]),df_binned['EM'].values, '*:',\n",
    "             color = colors[0], label = 'F1 (Strict)')\n",
    "    plt.plot(range(df_binned.shape[0]),df_binned['F1'].values, 'v--',\n",
    "             color = colors[0], label = 'F1 (Relaxed)')\n",
    "    plt.plot(range(df_binned.shape[0]),df_binned['Recall'].values, 'o-.',\n",
    "             color = colors[0], label = 'Recall (Relaxed)')\n",
    "    plt.plot(range(df_binned.shape[0]),df_binned['Precision'].values, 'X-',\n",
    "             color = colors[0], label = 'Precision (Relaxed)')\n",
    "    \n",
    "    plt.legend(loc = 'upper right', prop = {'size':12})\n",
    "#     best\n",
    "# \tupper right\n",
    "# \tupper left\n",
    "# \tlower left\n",
    "# \tlower right\n",
    "# \tright\n",
    "# \tcenter left\n",
    "# \tcenter right\n",
    "# \tlower center\n",
    "# \tupper center\n",
    "# \tcenter\n",
    "    plt.xticks(range(df_binned.shape[0]), list(df_binned.index),\n",
    "               fontsize=13)\n",
    "    plt.yticks(fontsize=13, color=colors[0])\n",
    "    \n",
    "    plt.ylim(0., 1.1)\n",
    "    plt.xlabel(f'\\nbins of {xlab}', fontsize=15)\n",
    "    plt.ylabel(f'performance score\\n', fontsize=15, color=colors[1], alpha=1)\n",
    "    \n",
    "    ax.tick_params(direction='out', bottom=True, left=True,\n",
    "                   length=5, width=1)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "#     df_binned_sum['COUNT_log'] = np.log10(df_binned_sum['COUNT'])\n",
    "    ax2.bar(range(df_binned.shape[0]),df_binned_sum['COUNT'].values,\n",
    "            hatch = '///', edgecolor='black',\n",
    "            alpha=.4, width=.4,color=colors[1],\n",
    "           )\n",
    "    plt.ylabel(f'\\ncount', fontsize=15, color=colors[1], alpha=1)\n",
    "#     ax2.set_yscale(\"log\")\n",
    "    plt.ylim(0, 6000)\n",
    "#     plt.yticks(range(0, max(df_binned_sum['COUNT'].values)+2000, 2000))\n",
    "    plt.yticks(fontsize=13, color=colors[1])\n",
    "    plt.show()\n",
    "\n",
    "    print(df_binned_sum['COUNT'].values)\n",
    "    \n",
    "    return df_binned\n",
    "\n",
    "\n",
    "import scipy\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "def get_confidence_score_binary(df, metric):\n",
    "    data_cs = df[metric].values.tolist()\n",
    "    x = sum([1 for y in data_cs if y==1])\n",
    "    n = len(data_cs)\n",
    "    c = proportion_confint(count=x, nobs=n, alpha=0.05,\n",
    "                           method=\"binom_test\"\n",
    "                          )\n",
    "    c1 = c[0]\n",
    "    c2 = c[1]\n",
    "        \n",
    "    return \"{0:.2f}\".format(c1*100), \"{0:.2f}\".format(c2*100)\n",
    "\n",
    "\n",
    "def get_confidence_score_nonbinary(df, metric):\n",
    "    x = df[metric].values\n",
    "    m = x.mean()\n",
    "    s = x.std()\n",
    "    dof = len(x)-1\n",
    "    confidence = 0.95\n",
    "\n",
    "    t_crit = np.abs(scipy.stats.t.ppf((1-confidence)/2, dof))\n",
    "    c1 = m-s*t_crit/np.sqrt(len(x))\n",
    "    c2 = m+s*t_crit/np.sqrt(len(x))\n",
    "\n",
    "    return \"{0:.2f}\".format(c1*100), \"{0:.2f}\".format(c2*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_categories_dist(df_nov, 'nov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_categories_dist(df_jan, 'jan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_categories_dist(df_feb, 'feb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = ['#e69d00', '#56b3e9', '#009e74', \n",
    "          '#f0e442', '#0071b2', '#808080',\n",
    "          '#d55c00', '#cc79a7', '#000000']\n",
    "map_cat = {\n",
    "           'active/historical':'active/historical use',\n",
    "           'drug names':'drug names',\n",
    "           'existence of idu':'existence of IDU',\n",
    "           'last use':'last use',\n",
    "           'occasional/daily':'frequency of use',\n",
    "           'needle sharing':'risky needle-using behavior',\n",
    "           'skin popping':'skin popping',\n",
    "           'evidence track marks': 'visible signs of IDU', \n",
    "           'ssp':'harm reduction interventions',\n",
    "}\n",
    "\n",
    "cat_to_color = dict(zip(map_cat.values(), colors))\n",
    "\n",
    "def plot_categories_dist(ax, which_plot, df, month):\n",
    "    \n",
    "    df['CAT'] = df['CATEGORIES'].map(map_cat)\n",
    "\n",
    "#     random.seed(1)\n",
    "#     colors = random.sample(sns.color_palette('pastel'),9)\n",
    "    \n",
    "    labels = [x for x in df['CAT'].value_counts().keys()]\n",
    "    \n",
    "    colors = [cat_to_color[k] for k in df['CAT'].value_counts().keys()]\n",
    "    \n",
    "#     fig,ax = plt.subplots(figsize=(8,22))\n",
    "    if month=='feb':\n",
    "        explode = (0.01,0.01,0.02,0.03,0.04,0.05,0.2,0.3,\n",
    "#                    .5\n",
    "                  )\n",
    "    else:\n",
    "        explode = (0.01,0.01,0.02,0.03,0.04,0.05,0.2,0.3,\n",
    "                   .5\n",
    "                  )\n",
    "        \n",
    "    ax.pie(df['CAT'].value_counts().values,\n",
    "            labels = labels,\n",
    "            explode=explode,\n",
    "            colors=colors,\n",
    "            autopct='%1.2f%%',\n",
    "            shadow=False,\n",
    "            startangle=45,\n",
    "            textprops={'fontsize':15})\n",
    "    # plt.xticks(size=25)\n",
    "    \n",
    "\n",
    "    ax.text(-0.2, 1.1, which_plot, ha='left', va='top', transform=ax.transAxes, fontsize=20, weight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dftosave = pd.DataFrame(df_nov['CAT'].value_counts())\n",
    "# dftosave.reset_index(inplace=True)\n",
    "# dftosave['queryGroups'] = dftosave['CAT']\n",
    "# dftosave.drop(['CAT'], axis=1, inplace=True)\n",
    "# dftosave.to_csv('../IDU_PAPER/result_fig/queryGroup_distribution_nov.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # plt.subplots_adjust(hspace=1, wspace=5)\n",
    "\n",
    "# fig = plt.figure(figsize=(17, 10))\n",
    "\n",
    "# ax1 = plt.subplot(2,2,1)\n",
    "# ax2 = plt.subplot(2,2,2)\n",
    "# axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "# plot_categories_dist(axes[0], 'a', df_feb, 'feb')\n",
    "# plot_categories_dist(axes[1], 'b', df_nov, 'nov')\n",
    "\n",
    "# fig.savefig(f\"/mnt/pfb_rvdata/mahbub/EGRESS/FIGURES/pie_feb_nov.svg\",\n",
    "#         format='svg', dpi=600,\n",
    "#         bbox_inches='tight'\n",
    "#        )\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# col = 'CONTEXT_LEN_WORDS'\n",
    "# ylab = 'context length'\n",
    "\n",
    "# col = 'QUESTION_LEN_WORDS'\n",
    "# ylab = 'question length'\n",
    "\n",
    "col = 'GANSWER_LEN_WORDS'\n",
    "ylab = 'answer length'\n",
    "\n",
    "stat = data[col].describe()\n",
    "bins = [\n",
    "    stat['min']-1,\n",
    "    stat['25%'],\n",
    "    stat['50%'],\n",
    "    stat['75%'],\n",
    "    stat['max'],\n",
    "    np.inf\n",
    "]\n",
    "\n",
    "# plot_binned_score_len(data, col, bins, f\"{ylab} (in number of words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_multiple(column_name, ylab, xlab, ax, which_plot, rotation=0):\n",
    "    \n",
    "    data['COUNT'] = [1]*len(data)\n",
    "\n",
    "    stat = data[column_name].describe()\n",
    "\n",
    "    bins = [\n",
    "        stat['min']-1,\n",
    "        stat['25%'],\n",
    "        stat['50%'],\n",
    "        stat['75%'],\n",
    "        stat['max'],\n",
    "        np.inf\n",
    "    ]\n",
    "\n",
    "\n",
    "    col = ['EM','F1','Precision','Recall',\n",
    "           'COUNT', column_name]\n",
    "\n",
    "    ytcklbl = [round(x,1) for x in np.arange(0,1.2,0.2)]\n",
    "\n",
    "    data_binned     = data.groupby(pd.cut(data[column_name],bins=bins))[col].mean()\n",
    "    data_binned_sum = data.groupby(pd.cut(data[column_name],bins=bins))[col].sum()\n",
    "\n",
    "    assert (data_binned.index == data_binned_sum.index).all()\n",
    "    assert data_binned.shape == data_binned_sum.shape\n",
    "\n",
    "    data_binned.dropna(inplace=True)\n",
    "    data_binned_sum.dropna(inplace=True)\n",
    "    data_binned_sum = data_binned_sum[data_binned_sum['COUNT']!=0]\n",
    "\n",
    "    assert data_binned.shape == data_binned_sum.shape\n",
    "\n",
    "    dftosave = data_binned\n",
    "    dftosave['COUNT'] = data_binned_sum['COUNT']\n",
    "    dftosave['Bins'] = dftosave.index.tolist()\n",
    "    dftosave.drop([column_name], axis=1, inplace=True)\n",
    "    dftosave.to_csv(f\"../IDU_PAPER/result_fig/error_{'_'.join(ylab.split())}.csv\", index=False)\n",
    "\n",
    "    ax.plot(range(data_binned.shape[0]),data_binned['EM'].values, '*:',\n",
    "             color = colors[0], label = 'F1 (Strict)')\n",
    "    ax.plot(range(data_binned.shape[0]),data_binned['F1'].values, 'v--',\n",
    "             color = colors[0], label = 'F1 (Relaxed)')\n",
    "    ax.plot(range(data_binned.shape[0]),data_binned['Recall'].values, 'o-.',\n",
    "             color = colors[0], label = 'Recall (Relaxed)')\n",
    "    ax.plot(range(data_binned.shape[0]),data_binned['Precision'].values, 'X-',\n",
    "             color = colors[0], label = 'Precision (Relaxed)')\n",
    "\n",
    "    ax.legend(loc = 'upper right',\n",
    "              prop = {'size':14}\n",
    "             )\n",
    "\n",
    "    ax.set_xticks(range(data_binned.shape[0]), list(data_binned.index),\n",
    "               fontsize=15, rotation=rotation\n",
    "                 )\n",
    "    ax.set_yticks(np.arange(0,1.2,0.2), ytcklbl,\n",
    "                  fontsize=15,\n",
    "                  color=colors[0])\n",
    "\n",
    "    ax.set_ylim(0., 1.1)\n",
    "    ax.set_xlabel(xlab,\n",
    "                  fontsize=17\n",
    "                 )\n",
    "    ax.set_ylabel(f'performance score',\n",
    "                  fontsize=17,\n",
    "                  color='#000000', alpha=1)\n",
    "\n",
    "    ax.tick_params(direction='out', bottom=True, left=True,\n",
    "                   length=5, width=1)\n",
    "\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    #     data_binned_sum['COUNT_log'] = np.log10(data_binned_sum['COUNT'])\n",
    "    ax2.bar(range(data_binned.shape[0]),data_binned_sum['COUNT'].values,\n",
    "            hatch = '///', edgecolor='black',\n",
    "            alpha=0.8, width=.4,color=colors[1],\n",
    "           )\n",
    "    ax2.set_ylabel(f'count',\n",
    "                   fontsize=17,\n",
    "                   color='#000000', alpha=1)\n",
    "\n",
    "    ax2.set_ylim(0, 8000)\n",
    "    #     plt.yticks(range(0, max(data_binned_sum['COUNT'].values)+2000, 2000))\n",
    "    ax2.set_yticks(range(1000, 9000, 1000), range(1000, 9000, 1000),\n",
    "                   fontsize=15,\n",
    "                   color=colors[1])\n",
    "\n",
    "    ax.text(-0.18, 1.1, which_plot, ha='left', va='top', transform=ax.transAxes, fontsize=20, weight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,6))\n",
    "# ax1 = plt.subplot(2,3,1)\n",
    "# ax2 = plt.subplot(2,3,2)\n",
    "# ax3 = plt.subplot(2,3,3)\n",
    "# ax4 = plt.subplot(2,1,2)\n",
    "# axes = [ax1, ax2, ax3, ax4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### fig, axes = plt.subplots(ncols=3, nrows = 2, figsize=(12,10),\n",
    "#                          constrained_layout=True,\n",
    "#                          gridspec_kw = {'width_ratios':[1,1,1],\n",
    "#                                         'height_ratios':[3,1]\n",
    "#                                        }\n",
    "#                         )\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=2)\n",
    "\n",
    "colors= ['#000000', '#0072b2']\n",
    "\n",
    "fig = plt.figure(figsize=(22, 15))\n",
    "\n",
    "# fig = plt.figure(figsize=(17, 9))\n",
    "# ax1 = plt.subplot(1,1,1)\n",
    "\n",
    "ax1 = plt.subplot(2,3,1)\n",
    "ax2 = plt.subplot(2,3,2)\n",
    "ax3 = plt.subplot(2,3,3)\n",
    "ax4 = plt.subplot(2,1,2)\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "# plt.ion()\n",
    "# fig.canvas.draw()\n",
    "\n",
    "# for ii, ax in enumerate(axes):\n",
    "#     bbox = ax.get_tightbbox(fig.canvas.get_renderer())\n",
    "#     x0,y0,width, height = bbox.transformed(fig.transFigure.inverted()).bounds\n",
    "    \n",
    "#     xpad = 0.007 * width\n",
    "#     ypad = 0.007 * height\n",
    "    \n",
    "#     fig.add_artist(plt.Rectangle((x0-xpad, y0-ypad),\n",
    "#                                  width+1.1*xpad, height+1.1*ypad,\n",
    "#                                  edgecolor='k', linewidth=.5,\n",
    "#                                  fill=False))\n",
    "    \n",
    "#################################################################################################\n",
    "ax = axes[0]\n",
    "\n",
    "column_name = 'CONTEXT_LEN_WORDS'\n",
    "ylab = 'context length'\n",
    "which_plot = 'a'\n",
    "\n",
    "xlab = f'bins of {ylab} (in number of words)'\n",
    "\n",
    "plot_multiple(column_name, ylab, xlab, ax, which_plot,rotation=10)\n",
    "\n",
    "##################################################################################################\n",
    "ax = axes[1]\n",
    "\n",
    "column_name = 'QUESTION_LEN_WORDS'\n",
    "ylab = 'question length'\n",
    "which_plot = 'b'\n",
    "\n",
    "xlab = f'\\nbins of {ylab} (in number of words)'\n",
    "\n",
    "plot_multiple(column_name, ylab, xlab, ax, which_plot,rotation=0)\n",
    "\n",
    "##################################################################################################\n",
    "ax = axes[2]\n",
    "\n",
    "column_name = 'GANSWER_LEN_WORDS'\n",
    "ylab = 'answer length'\n",
    "which_plot = 'c'\n",
    "\n",
    "xlab = f'\\nbins of {ylab} (in number of words)'\n",
    "\n",
    "plot_multiple(column_name, ylab, xlab, ax, which_plot,rotation=0)\n",
    "\n",
    "##################################################################################################\n",
    "ax = axes[3]\n",
    "\n",
    "ax.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['EM'].values,\n",
    "    '*', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'F1 (Strict)')\n",
    "ax.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['F1'].values,\n",
    "    'v', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'F1 (Relaxed)')\n",
    "ax.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['Recall'].values,\n",
    "    'o', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'Recall (Relaxed)')\n",
    "ax.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['Precision'].values,\n",
    "    'X', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'Precision (Relaxed)')\n",
    "\n",
    "\n",
    "ax.legend(loc = 'upper right', prop = {'size':16}, facecolor=\"white\", framealpha=1)\n",
    "\n",
    "ax.set_xticks(range(len(df_cat_final)),\n",
    "           xticklab,\n",
    "              fontsize=17\n",
    "             )\n",
    "\n",
    "\n",
    "ax.grid(axis='x', color=colors[0], alpha=.2)\n",
    "ax.set_yticks(np.arange(0, 1.1, .2), ytcklbl,\n",
    "              fontsize=17,\n",
    "              color=colors[0])\n",
    "ax.set_ylim(-.4, 1.25)\n",
    "ax.set_xlabel(f'query groups',\n",
    "              fontsize=19\n",
    "             )\n",
    "ax.set_ylabel(f'performance score',\n",
    "              fontsize=19,\n",
    "              color='#000000', alpha=1)\n",
    "\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "f = ax2.bar(df_cat_final['CATEGORIES'].values, df_cat_final['Count'].values,\n",
    "            hatch = '///', edgecolor='k',\n",
    "            alpha=.8, width=.4,color=colors[1],\n",
    "            log=True\n",
    "       )\n",
    "\n",
    "# ax2.tick_params(direction='out', bottom=True, left=True,\n",
    "#                length=5, width=1)\n",
    "\n",
    "# ax2.set_xticks(range(len(df_cat_final)), xticklab,\n",
    "#                fontsize=15\n",
    "#               )\n",
    "\n",
    "ax2.set_xlabel(f'query groups', \n",
    "               fontsize=19\n",
    "              )\n",
    "ax2.set_ylabel(f'count (log scale)',\n",
    "               fontsize=19,\n",
    "               color='#000000', alpha=1)\n",
    "ax2.set_ylim(0, 400000)\n",
    "# xtck = []\n",
    "plt.yticks(fontsize=17, color=colors[1])\n",
    "\n",
    "ax.text(-0.05, 1.1, 'd', ha='left', va='top', transform=ax.transAxes, fontsize=20, weight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# fig.savefig(\"/mnt/pfb_rvdata/mahbub/EGRESS/FIGURES/error.svg\",\n",
    "#             format='svg', dpi=600,\n",
    "#             bbox_inches='tight'\n",
    "#            )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.read_csv('../IDU_PAPER/result_fig/queryGroup_distribution_feb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_result_table(model_name, SUFFIX):\n",
    "    \n",
    "    CACHED_PATH = f\"/mnt/pfb_rvdata/mahbub/OUTPUT/post_validation/{model_name}/tokenized_data/\"\n",
    "    RESULT_PATH = f\"/mnt/pfb_rvdata/mahbub/OUTPUT/post_validation/{model_name}_5epochs/test_results{SUFFIX}/\"\n",
    "\n",
    "    df_res = pd.read_csv(f\"{RESULT_PATH}results_{model_name}_512_idu_mrc{SUFFIX}.csv\")\n",
    "\n",
    "    df_res['qas_id'] = df_res['qas_id'].apply(str)\n",
    "\n",
    "    cached_path_jan             = f\"{CACHED_PATH}cached_dev_{model_name}_512_test_idu_mrc{SUFFIX}\"\n",
    "    features_and_dataset        = torch.load(cached_path_jan)\n",
    "    features, dataset, examples = (features_and_dataset['features'],\n",
    "                                   features_and_dataset['dataset'],\n",
    "                                   features_and_dataset['examples'],)\n",
    "\n",
    "    id_to_gans = {}\n",
    "    id_to_cont  = {}\n",
    "    id_to_ques  = {}\n",
    "    for ex in examples:\n",
    "        id_to_gans[str(ex.__dict__['qas_id'])] = ex.__dict__['answers'][0]['text']\n",
    "        id_to_cont[str(ex.__dict__['qas_id'])] = ex.__dict__['context_text']\n",
    "        id_to_ques[str(ex.__dict__['qas_id'])] = ex.__dict__['question_text']\n",
    "\n",
    "    id_to_pans = json.load(\n",
    "        open(\n",
    "            f'{RESULT_PATH}predictions_idu_mrc{SUFFIX}.json'))\n",
    "\n",
    "    df_res['Context']      = df_res['qas_id'].map(id_to_cont)\n",
    "    df_res['Question']     = df_res['qas_id'].map(id_to_ques)\n",
    "    df_res['Ground_truth'] = df_res['qas_id'].map(id_to_gans)\n",
    "    df_res['Prediction']   = df_res['qas_id'].map(id_to_pans)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no_ans_pred = json.load(open(\"/mnt/pfb_rvdata/mahbub/OUTPUT/post_validation/clinicalbert_5epochs/test_results_jan_no_ans/predictions_idu_mrc_jan_no_ans.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# res_no_ans = get_result_table('clinicalbert', '_jan_no_ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp = res_no_ans[res_no_ans['Question']==list(set(res_no_ans['Question']))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp.shape, temp['Question'].iloc[0], temp['Prediction'].value_counts()/443*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# no_ans_output = Counter(list(no_ans_pred.values()))\n",
    "# no_ans_output = {k:round(v/sum(no_ans_output.values())*100, 2) for k,v in no_ans_output.items()}\n",
    "# no_ans_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_jan_res = get_result_table(\"clinicalbert\", \"_jan\")\n",
    "df_feb_res = get_result_table(\"clinicalbert\", \"_feb\")\n",
    "df_nov_res = get_result_table(\"clinicalbert\", \"_nov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_jan_res.to_csv(\"/mnt/pfb_rvdata/mahbub/machine_reading_comprehension/additional_testing/output/results_jan2022_hepC.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results for January ......... \\n\")\n",
    "print(f\"Strict F1        : {round(df_jan_res['EM'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed F1       : {round(df_jan_res['F1'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed Precision: {round(df_jan_res['Precision'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed Recall   : {round(df_jan_res['Recall'].mean()*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results for February ......... \\n\")\n",
    "print(f\"Strict F1        : {round(df_feb_res['EM'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed F1       : {round(df_feb_res['F1'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed Precision: {round(df_feb_res['Precision'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed Recall   : {round(df_feb_res['Recall'].mean()*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results for November ......... \\n\")\n",
    "print(f\"Strict F1        : {round(df_nov_res['EM'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed F1       : {round(df_nov_res['F1'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed Precision: {round(df_nov_res['Precision'].mean()*100, 2)}\")\n",
    "print(f\"Relaxed Recall   : {round(df_nov_res['Recall'].mean()*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(get_confidence_score_binary(df_nov_res, 'EM'))\n",
    "print(get_confidence_score_nonbinary(df_nov_res, 'F1'))\n",
    "print(get_confidence_score_nonbinary(df_nov_res, 'Precision'))\n",
    "print(get_confidence_score_nonbinary(df_nov_res, 'Recall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = df_jan_res\n",
    "data = pd.concat([df_jan_res, df_feb_res, df_nov_res])\n",
    "# data = pd.concat(data, df_nov_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape, df_jan_res.shape, df_feb_res.shape, df_nov_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3235+1985+1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['CONTEXT_LEN_TOKENS'] = data['Context'].progress_apply(count_tokens)\n",
    "data['QUESTION_LEN_TOKENS'] = data['Question'].progress_apply(count_tokens)\n",
    "data['GANSWER_LEN_TOKENS'] = data['Ground_truth'].progress_apply(count_tokens)\n",
    "data['PANSWER_LEN_TOKENS'] = data['Prediction'].progress_apply(count_tokens)\n",
    "\n",
    "data['CONTEXT_LEN_WORDS'] = data['Context'].progress_apply(count_words)\n",
    "data['QUESTION_LEN_WORDS'] = data['Question'].progress_apply(count_words)\n",
    "data['GANSWER_LEN_WORDS'] = data['Ground_truth'].progress_apply(count_words)\n",
    "data['PANSWER_LEN_WORDS'] = data['Prediction'].progress_apply(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # col = 'CONTEXT_LEN_WORDS'\n",
    "# # ylab = 'context length'\n",
    "\n",
    "# col = 'QUESTION_LEN_WORDS'\n",
    "# ylab = 'question length'\n",
    "\n",
    "# # # col = 'GANSWER_LEN_WORDS'\n",
    "# # # ylab = 'answer length'\n",
    "\n",
    "# stat = data[col].describe()\n",
    "# bins = [\n",
    "#     stat['min']-1,\n",
    "#     stat['25%'],\n",
    "#     stat['50%'],\n",
    "#     stat['75%'],\n",
    "#     stat['max'],\n",
    "#     np.inf\n",
    "# ]\n",
    "\n",
    "# plot_binned_score_len(data, col, bins, f\"{ylab} (in number of words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cat_ques = pd.concat([df_jan[['CATEGORIES','QUESTION']],\n",
    "                     df_feb[['CATEGORIES','QUESTION']],\n",
    "                     df_nov[['CATEGORIES','QUESTION']]])\n",
    "df_cat_ques = df_cat_ques.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_cat_ques['CAT'] = df_cat_ques['CATEGORIES'].map(map_cat)\n",
    "\n",
    "# df_cat_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ques_to_cat = dict(zip(df_cat_ques['QUESTION'], df_cat_ques['CAT']))\n",
    "data['CATEGORIES'] = data['Question'].map(ques_to_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['CATEGORIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_cat_order = [\n",
    "    'drug names',\n",
    "    'visible signs of IDU',\n",
    "    'risky needle-using behavior',\n",
    "    'active/historical use',\n",
    "    'frequency of use',\n",
    "    'last use',\n",
    "    'skin popping',\n",
    "    'harm reduction interventions',\n",
    "    'existence of IDU',\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp1 = data[data['CATEGORIES']==fixed_cat_order[8]]\n",
    "# tmp1.groupby(['Question']).sum()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cat = data[['CATEGORIES',\n",
    "             'EM', 'Recall', 'F1', 'Precision',\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cat = data['CATEGORIES'].value_counts().rename_axis('CATEGORIES').reset_index(name='counts')\n",
    "# df_cat = data.reset_index().groupby(['CATEGORIES']).mean()\n",
    "# df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat = data_cat['CATEGORIES'].value_counts().rename_axis('CATEGORIES').reset_index(name='counts')\n",
    "\n",
    "df_cat = data_cat.reset_index().groupby(['CATEGORIES']).mean()\n",
    "df_cat = df_cat.reindex(cat['CATEGORIES'].values.tolist())\n",
    "\n",
    "em_dict = dict(zip(df_cat.index, df_cat['EM']))\n",
    "re_dict = dict(zip(df_cat.index, df_cat['Recall']))\n",
    "pr_dict = dict(zip(df_cat.index, df_cat['Precision']))\n",
    "f1_dict = dict(zip(df_cat.index, df_cat['F1']))\n",
    "cat_dict = dict(zip(cat['CATEGORIES'], cat['counts']))\n",
    "\n",
    "df_cat_final = pd.DataFrame()\n",
    "df_cat_final['CATEGORIES'] = fixed_cat_order\n",
    "df_cat_final['EM']        = df_cat_final['CATEGORIES'].map(em_dict)\n",
    "df_cat_final['Recall']    = df_cat_final['CATEGORIES'].map(re_dict)\n",
    "df_cat_final['Precision'] = df_cat_final['CATEGORIES'].map(pr_dict)\n",
    "df_cat_final['F1']        = df_cat_final['CATEGORIES'].map(f1_dict)\n",
    "df_cat_final['Count']     = df_cat_final['CATEGORIES'].map(cat_dict) \n",
    "# df_cat_final['Count_log2'] = np.log2(df_cat_final['Count'])\n",
    "\n",
    "# df_cat_final['EM'] = df_cat_final['EM'].fillna(10)\n",
    "# df_cat_final['PR'] = df_cat_final['PR'].fillna(10)\n",
    "# df_cat_final['F1'] = df_cat_final['F1'].fillna(10)\n",
    "# df_cat_final['Count'] = df_cat_final['Count'].fillna(0)\n",
    "\n",
    "df_cat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xticklab = [\n",
    "            'drug\\nnames',\n",
    "            'visible signs\\nof IDU',\n",
    "            'risky\\nneedle-using\\nbehavior',\n",
    "            'active/\\nhistorical\\nuse',\n",
    "            'frequency\\nof use',\n",
    "            'last\\nuse',\n",
    "            'skin\\npopping',\n",
    "            'harm\\nreduction\\ninterventions',\n",
    "            'existence\\nof IDU',\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "colors= ['#e69f00', '#000000']\n",
    "# sns.color_palette('dark')\n",
    "\n",
    "plt.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['EM'].values,\n",
    "    '*', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'F1 (Strict)')\n",
    "plt.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['F1'].values,\n",
    "    'v', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'F1 (Relaxed)')\n",
    "plt.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['Recall'].values,\n",
    "    'o', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'Recall (Relaxed)')\n",
    "plt.plot(\n",
    "    range(len(df_cat_final)), df_cat_final['Precision'].values,\n",
    "    'X', \n",
    "    markersize = 10,\n",
    "    color = colors[0],\n",
    "    label = 'Precision (Relaxed)')\n",
    "\n",
    "\n",
    "plt.legend(loc = 'upper right', prop = {'size':12}, facecolor=\"white\", framealpha=1)\n",
    "\n",
    "plt.xticks(range(len(df_cat_final)),\n",
    "           xticklab, fontsize=13)\n",
    "\n",
    "\n",
    "plt.grid(axis='x', color=colors[0], alpha=.2)\n",
    "plt.yticks(np.arange(0, 1.1, .2), fontsize=13, color=colors[0])\n",
    "plt.ylim(-.4, 1.25)\n",
    "plt.xlabel(f'\\nquery groups', fontsize=15)\n",
    "plt.ylabel(f'performance score\\n', fontsize=15, color='#000000', alpha=1)\n",
    "\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "f = ax2.bar(df_cat_final['CATEGORIES'].values, df_cat_final['Count'].values,\n",
    "            hatch = '///', edgecolor='k',\n",
    "            alpha=.4, width=.4,color=colors[1],\n",
    "            log=True\n",
    "       )\n",
    "\n",
    "ax.tick_params(direction='out', bottom=True, left=True,\n",
    "               length=5, width=1)\n",
    "\n",
    "plt.xticks(range(len(df_cat_final)), xticklab, fontsize=13)\n",
    "\n",
    "plt.xlabel(f'\\nquery groups', fontsize=15)\n",
    "plt.ylabel(f'\\ncount (log scale)', fontsize=15, color='#000000', alpha=1)\n",
    "plt.ylim(0, 400000)\n",
    "plt.yticks(fontsize=13, color=colors[1])\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\"/mnt/pfb_rvdata/mahbub/EGRESS/FIGURES/error_querygroup.svg\",\n",
    "#             format='svg', dpi=600,\n",
    "#             bbox_inches='tight'\n",
    "#            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tiusid_to_notetype = pickle.load(open(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/tiusid_to_notetype.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tiu1 = pd.read_csv(\n",
    "    \"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/train_test_jan_postvalidation.csv\")[['CONTEXT','TIUDocumentSID']]\n",
    "tiu2 = pd.read_csv(\n",
    "    \"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_feb_postvalidation.csv\")[['CONTEXT','TIUDocumentSID']]\n",
    "tiu3 = pd.read_csv(\"/mnt/pfb_rvdata/mahbub/preprocessed_data/final_validated_data/test_nov_postvalidation.csv\")[['CONTEXT','TIUDocumentSID']]\n",
    "\n",
    "df_tiusid_note = pd.concat([tiu1, tiu2, tiu3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tiusid_note.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "note_to_tiusid = dict(zip(df_tiusid_note['CONTEXT'], df_tiusid_note['TIUDocumentSID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['TIUDocumentSID'] = data['Context'].map(note_to_tiusid)\n",
    "data['Note Type']      = data['TIUDocumentSID'].map(tiusid_to_notetype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_ntype_order = data['Note Type'].value_counts().keys()\n",
    "xticklab = data['Note Type'].value_counts().keys()\n",
    "\n",
    "ntype = data['Note Type'].value_counts().rename_axis('Note Type').reset_index(name='counts')\n",
    "\n",
    "df_ntype = data.reset_index().groupby(['Note Type']).mean()\n",
    "df_ntype = df_ntype.reindex(ntype['Note Type'].values.tolist())\n",
    "\n",
    "em_dict = dict(zip(df_ntype.index, df_ntype['EM']))\n",
    "re_dict = dict(zip(df_ntype.index, df_ntype['Recall']))\n",
    "pr_dict = dict(zip(df_ntype.index, df_ntype['Precision']))\n",
    "f1_dict = dict(zip(df_ntype.index, df_ntype['F1']))\n",
    "ntype_dict = dict(zip(ntype['Note Type'], ntype['counts']))\n",
    "\n",
    "df_ntype_final = pd.DataFrame()\n",
    "df_ntype_final['Note Type'] = fixed_ntype_order\n",
    "df_ntype_final['EM']        = df_ntype_final['Note Type'].map(em_dict)\n",
    "df_ntype_final['Recall']    = df_ntype_final['Note Type'].map(re_dict)\n",
    "df_ntype_final['Precision'] = df_ntype_final['Note Type'].map(pr_dict)\n",
    "df_ntype_final['F1']        = df_ntype_final['Note Type'].map(f1_dict)\n",
    "df_ntype_final['Count']     = df_ntype_final['Note Type'].map(ntype_dict) \n",
    "# df_ntype_final['Count_log2'] = np.log2(df_ntype_final['Count'])\n",
    "\n",
    "# df_ntype_final['EM'] = df_ntype_final['EM'].fillna(10)\n",
    "# df_ntype_final['PR'] = df_ntype_final['PR'].fillna(10)\n",
    "# df_ntype_final['F1'] = df_ntype_final['F1'].fillna(10)\n",
    "# df_ntype_final['Count'] = df_ntype_final['Count'].fillna(0)\n",
    "\n",
    "df_ntype_final = df_ntype_final.head(50)\n",
    "xticklab = xticklab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(14,8))\n",
    "# colors=sns.color_palette('dark')\n",
    "\n",
    "# plt.plot(\n",
    "#     range(len(df_ntype_final)), df_ntype_final['EM'].values,\n",
    "#     '*', \n",
    "#     markersize = 5,\n",
    "#     color = colors[0],\n",
    "#     label = 'F1 (Strict)')\n",
    "# plt.plot(\n",
    "#     range(len(df_ntype_final)), df_ntype_final['F1'].values,\n",
    "#     'v', \n",
    "#     markersize = 5,\n",
    "#     color = colors[0],\n",
    "#     label = 'F1 (Relaxed)')\n",
    "# plt.plot(\n",
    "#     range(len(df_ntype_final)), df_ntype_final['Recall'].values,\n",
    "#     'o', \n",
    "#     markersize = 5,\n",
    "#     color = colors[0],\n",
    "#     label = 'Recall (Relaxed)')\n",
    "# plt.plot(\n",
    "#     range(len(df_ntype_final)), df_ntype_final['Precision'].values,\n",
    "#     'X', \n",
    "#     markersize = 5,\n",
    "#     color = colors[0],\n",
    "#     label = 'Precision (Relaxed)')\n",
    "\n",
    "\n",
    "# plt.legend(loc = 'upper right', prop = {'size':12}, facecolor=\"white\", framealpha=1)\n",
    "\n",
    "# plt.xticks(range(len(df_ntype_final)),\n",
    "#            xticklab, fontsize=13, rotation=90)\n",
    "\n",
    "\n",
    "# plt.grid(axis='x', color=colors[0], alpha=.2)\n",
    "# plt.yticks(np.arange(0, 1.1, .2), fontsize=13, color=colors[0])\n",
    "# plt.ylim(-.4, 1.25)\n",
    "# plt.xlabel(f'\\nQuery Groups', fontsize=15)\n",
    "# plt.ylabel(f'Performance Score\\n', fontsize=15, color='#000000', alpha=1)\n",
    "\n",
    "\n",
    "# ax2 = ax.twinx()\n",
    "\n",
    "# f = ax2.bar(df_ntype_final['Note Type'].values, df_ntype_final['Count'].values,\n",
    "#             hatch = '///', edgecolor='black',\n",
    "#             alpha=.4, width=.4,color=colors[2],\n",
    "#             log=True\n",
    "#        )\n",
    "\n",
    "# ax.tick_params(direction='out', bottom=True, left=True,\n",
    "#                length=5, width=1)\n",
    "\n",
    "# plt.xticks(range(len(df_ntype_final)), xticklab, fontsize=13)\n",
    "\n",
    "# plt.xlabel(f'\\nQuery Groups', fontsize=15)\n",
    "# plt.ylabel(f'\\nCount (log scale)', fontsize=15, color='#000000', alpha=1)\n",
    "# plt.ylim(0, 400000)\n",
    "# plt.yticks(fontsize=13, color=colors[2])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_jan_res[df_jan_res['Recall']==1].shape[0]/len(df_jan_res))\n",
    "print(df_feb_res[df_feb_res['Recall']==1].shape[0]/len(df_feb_res))\n",
    "print(df_nov_res[df_nov_res['Recall']==1].shape[0]/len(df_nov_res))\n",
    "\n",
    "print(df_jan_res[df_jan_res['Precision']==1].shape[0]/len(df_jan_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(df_feb_res[(df_feb_res['Recall']>.3) & (df_feb_res['Recall']<=.4)]['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    df_feb_res[df_feb_res['EM']!=1]['Recall'].value_counts(\n",
    "        bins = [-.001, 0, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99,1]\n",
    "    )/df_feb_res.shape[0]*100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "445/df_jan_res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['CONTEXT_LEN_TOKENS'] = data['Context'].progress_apply(count_tokens)\n",
    "data['QUESTION_LEN_TOKENS'] = data['Question'].progress_apply(count_tokens)\n",
    "data['GANSWER_LEN_TOKENS'] = data['Ground_truth'].progress_apply(count_tokens)\n",
    "data['PANSWER_LEN_TOKENS'] = data['Prediction'].progress_apply(count_tokens)\n",
    "\n",
    "data['CONTEXT_LEN_WORDS'] = data['Context'].progress_apply(count_words)\n",
    "data['QUESTION_LEN_WORDS'] = data['Question'].progress_apply(count_words)\n",
    "data['GANSWER_LEN_WORDS'] = data['Ground_truth'].progress_apply(count_words)\n",
    "data['PANSWER_LEN_WORDS'] = data['Prediction'].progress_apply(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['GT_CONTEXT_RATIO']   = data['GANSWER_LEN_WORDS']/data['CONTEXT_LEN_WORDS']\n",
    "data['PRED_CONTEXT_RATIO'] = data['PANSWER_LEN_WORDS']/data['CONTEXT_LEN_WORDS']\n",
    "\n",
    "data['GT_CONTEXT_RATIO'] = data['GT_CONTEXT_RATIO']*100\n",
    "data['PRED_CONTEXT_RATIO'] = data['PRED_CONTEXT_RATIO']*100\n",
    "\n",
    "\n",
    "df_recall_only = data[data['Recall'] == 1]\n",
    "print(data[data['EM'] == 1].shape)\n",
    "print(df_recall_only.shape)\n",
    "\n",
    "df_recall_only = df_recall_only[df_recall_only['EM'] != 1]\n",
    "print(df_recall_only.shape)\n",
    "\n",
    "temp1 = df_recall_only[['GT_CONTEXT_RATIO']]\n",
    "temp2 = df_recall_only[['PRED_CONTEXT_RATIO']]\n",
    "\n",
    "temp1['Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)'] = temp1['GT_CONTEXT_RATIO']\n",
    "temp2['Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)'] = temp2['PRED_CONTEXT_RATIO']\n",
    "\n",
    "temp1.drop('GT_CONTEXT_RATIO', axis=1, inplace=True)\n",
    "temp2.drop('PRED_CONTEXT_RATIO', axis=1, inplace=True)\n",
    "\n",
    "temp1['Answers'] = ['Ground Truths']*len(temp1)\n",
    "temp2['Answers'] = ['Predictions']*len(temp2)\n",
    "\n",
    "temp = pd.concat([temp1, temp2])\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp['ratio of answer to context (%)'] = temp['Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)']\n",
    "# temp.drop(['Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)'], axis=1, inplace=True)\n",
    "\n",
    "# temp.to_csv(\"../IDU_PAPER/result_fig/ratio.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# color=random.sample(sns.color_palette('pastel'), 2)\n",
    "# color = [green, blue]\n",
    "# color = [(0.5529411764705883, 0.8980392156862745, 0.6313725490196078),\n",
    "#          (0.6313725490196078, 0.788235294117647, 0.9568627450980393)\n",
    "#         ]\n",
    "# colors= ['#e69f00', '#808080']\n",
    "\n",
    "colors= ['#808080', '#0072b2']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "\n",
    "sns.boxplot(data = temp, palette=colors, #alpha=0.3,\n",
    "            x = \"Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)\", y = \"Answers\")\n",
    "plt.axvline(x=0, alpha=0.2, linestyle='--', color = 'k')\n",
    "plt.xlim(-.4, 4)\n",
    "plt.yticks(range(2),['gold-standard', 'prediction'], fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"\\nratio of answer to context (%)\", fontsize=13)\n",
    "#            \\n (considering only cases where \n",
    "#                predicted answers do not have\\n \n",
    "#                exact match but 100% overlap with ground truth answers)\n",
    "plt.ylabel(\"answers\", fontsize=13)\n",
    "plt.show()\n",
    "# plt.tight_layout()\n",
    "# fig.savefig(\"/mnt/pfb_rvdata/mahbub/EGRESS/FIGURES/ratio.svg\",\n",
    "#             format='svg', dpi=600,\n",
    "#             bbox_inches='tight'\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp[temp['Answers']=='Ground Truths']['Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[temp['Answers']=='Prediction']['Ratio of Answer (w/ EM=0 & Acc.=1) to Context (%)'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recall_only.sort_values(by=['PRED_CONTEXT_RATIO'])[['GT_CONTEXT_RATIO', 'PRED_CONTEXT_RATIO']].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
